{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-07-15T12:08:30.515595700Z",
     "start_time": "2023-07-15T12:08:28.598715500Z"
    }
   },
   "outputs": [],
   "source": [
    "from scholarly import scholarly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'container_type': 'Author', 'filled': ['basics', 'indices', 'counts', 'coauthors', 'publications', 'public_access'], 'source': <AuthorSource.SEARCH_AUTHOR_SNIPPETS: 'SEARCH_AUTHOR_SNIPPETS'>, 'scholar_id': '-WjvWTkAAAAJ', 'url_picture': 'https://scholar.google.com/citations?view_op=medium_photo&user=-WjvWTkAAAAJ', 'name': 'Fabio Hellmann', 'affiliation': 'University of Augsburg', 'email_domain': '@uni-a.de', 'interests': ['Medical Imaging', 'Biosignal Processing', 'Explainable AI', 'Deep Learning', 'Human-Computer Interaction'], 'citedby': 5, 'homepage': 'https://www.uni-augsburg.de/de/fakultaet/fai/informatik/prof/hcm/team/hellmann/', 'citedby5y': 5, 'hindex': 2, 'hindex5y': 2, 'i10index': 0, 'i10index5y': 0, 'cites_per_year': {2022: 1, 2023: 4}, 'coauthors': [{'container_type': 'Author', 'filled': [], 'scholar_id': '5gnr0JUAAAAJ', 'source': <AuthorSource.CO_AUTHORS_LIST: 'CO_AUTHORS_LIST'>, 'name': 'Elisabeth Andre', 'affiliation': 'Professor of Computer Sciences, Augsburg University'}, {'container_type': 'Author', 'filled': [], 'scholar_id': 'pBv4kVEAAAAJ', 'source': <AuthorSource.CO_AUTHORS_LIST: 'CO_AUTHORS_LIST'>, 'name': 'Alexander Hustinx', 'affiliation': 'Institute for Genome Statistics and Bioinformatics, University Bonn'}, {'container_type': 'Author', 'filled': [], 'scholar_id': 'ynm_HZoAAAAJ', 'source': <AuthorSource.CO_AUTHORS_LIST: 'CO_AUTHORS_LIST'>, 'name': 'Prof. Dr. med., Dipl. Phys. Peter Krawitz', 'affiliation': 'Institute for Genomic Statistics and Bioinformatics'}, {'container_type': 'Author', 'filled': [], 'scholar_id': 'XNXSgisAAAAJ', 'source': <AuthorSource.CO_AUTHORS_LIST: 'CO_AUTHORS_LIST'>, 'name': 'Tzung-Chien Hsieh', 'affiliation': 'Institute for Genome Statistics and Bioinformatics, University Bonn'}, {'container_type': 'Author', 'filled': [], 'scholar_id': 'Klrd5CQAAAAJ', 'source': <AuthorSource.CO_AUTHORS_LIST: 'CO_AUTHORS_LIST'>, 'name': 'Zhao Ren', 'affiliation': 'Univeristy of Bremen'}, {'container_type': 'Author', 'filled': [], 'scholar_id': 'TxKNCSoAAAAJ', 'source': <AuthorSource.CO_AUTHORS_LIST: 'CO_AUTHORS_LIST'>, 'name': 'Björn Schuller', 'affiliation': 'Professor of AI, Imperial College London & University of Augsburg & CSO, audEERING'}], 'publications': [{'container_type': 'Publication', 'source': <PublicationSource.AUTHOR_PUBLICATION_ENTRY: 'AUTHOR_PUBLICATION_ENTRY'>, 'bib': {'title': 'Improving Deep Facial Phenotyping for Ultra-rare Disorder Verification Using Model Ensembles', 'pub_year': '2023', 'citation': 'Proceedings of the IEEE/CVF Winter Conference on Applications of Computer …, 2023'}, 'filled': False, 'author_pub_id': '-WjvWTkAAAAJ:d1gkVwhDpl0C', 'num_citations': 3, 'citedby_url': 'https://scholar.google.com/scholar?oi=bibs&hl=en&cites=13773664767143044663', 'cites_id': ['13773664767143044663']}, {'container_type': 'Publication', 'source': <PublicationSource.AUTHOR_PUBLICATION_ENTRY: 'AUTHOR_PUBLICATION_ENTRY'>, 'bib': {'title': 'Deformable dilated faster R-CNN for universal lesion detection in CT images', 'pub_year': '2021', 'citation': '2021 43rd Annual International Conference of the IEEE Engineering in …, 2021'}, 'filled': False, 'author_pub_id': '-WjvWTkAAAAJ:u5HHmVD_uO8C', 'num_citations': 2, 'citedby_url': 'https://scholar.google.com/scholar?oi=bibs&hl=en&cites=10699106320797145046', 'cites_id': ['10699106320797145046']}, {'container_type': 'Publication', 'source': <PublicationSource.AUTHOR_PUBLICATION_ENTRY: 'AUTHOR_PUBLICATION_ENTRY'>, 'bib': {'title': 'The STOIC2021 COVID-19 AI challenge: applying reusable training methodologies to private data', 'pub_year': '2023', 'citation': 'arXiv preprint arXiv:2306.10484, 2023'}, 'filled': False, 'author_pub_id': '-WjvWTkAAAAJ:Tyk-4Ss8FVUC', 'num_citations': 0}, {'container_type': 'Publication', 'source': <PublicationSource.AUTHOR_PUBLICATION_ENTRY: 'AUTHOR_PUBLICATION_ENTRY'>, 'bib': {'title': 'Towards Automated COVID-19 Presence and Severity Classification', 'pub_year': '2023', 'citation': 'arXiv preprint arXiv:2305.08660, 2023'}, 'filled': False, 'author_pub_id': '-WjvWTkAAAAJ:zYLM7Y9cAGgC', 'num_citations': 0}, {'container_type': 'Publication', 'source': <PublicationSource.AUTHOR_PUBLICATION_ENTRY: 'AUTHOR_PUBLICATION_ENTRY'>, 'bib': {'title': 'GANonymization: A GAN-based Face Anonymization Framework for Preserving Emotional Expressions', 'pub_year': '2023', 'citation': 'arXiv preprint arXiv:2305.02143, 2023'}, 'filled': False, 'author_pub_id': '-WjvWTkAAAAJ:IjCSPb-OGe4C', 'num_citations': 0}, {'container_type': 'Publication', 'source': <PublicationSource.AUTHOR_PUBLICATION_ENTRY: 'AUTHOR_PUBLICATION_ENTRY'>, 'bib': {'title': 'Few-Shot Meta Learning for Recognizing Facial Phenotypes of Genetic Disorders', 'pub_year': '2022', 'citation': 'arXiv preprint arXiv:2210.12705, 2022'}, 'filled': False, 'author_pub_id': '-WjvWTkAAAAJ:u-x6o8ySG0sC', 'num_citations': 0}, {'container_type': 'Publication', 'source': <PublicationSource.AUTHOR_PUBLICATION_ENTRY: 'AUTHOR_PUBLICATION_ENTRY'>, 'bib': {'title': 'NFB-03. Neurological manifestations in children and adolescents with Neurofibromatosis type 1-Implications for management and surveillance', 'pub_year': '2022', 'citation': 'Neuro-Oncology 24 (Supplement_1), i128-i128, 2022'}, 'filled': False, 'author_pub_id': '-WjvWTkAAAAJ:9yKSN-GCB0IC', 'num_citations': 0}], 'public_access': {'available': 1, 'not_available': 0}}\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the author's data, fill-in, and print\n",
    "# Get an iterator for the author results\n",
    "search_query = scholarly.search_author('Fabio Hellmann')\n",
    "my_profile = list(filter(lambda author: author['scholar_id'] == '-WjvWTkAAAAJ', search_query))[0]\n",
    "my_profile = scholarly.fill(my_profile)\n",
    "print(my_profile)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-07-15T12:08:36.023080800Z",
     "start_time": "2023-07-15T12:08:30.515595700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: Improving Deep Facial Phenotyping for Ultra-rare Disorder Verification Using Model Ensembles\n",
      "2: Deformable dilated faster R-CNN for universal lesion detection in CT images\n",
      "3: The STOIC2021 COVID-19 AI challenge: applying reusable training methodologies to private data\n",
      "4: Towards Automated COVID-19 Presence and Severity Classification\n",
      "5: GANonymization: A GAN-based Face Anonymization Framework for Preserving Emotional Expressions\n",
      "6: Few-Shot Meta Learning for Recognizing Facial Phenotypes of Genetic Disorders\n",
      "7: NFB-03. Neurological manifestations in children and adolescents with Neurofibromatosis type 1-Implications for management and surveillance\n"
     ]
    },
    {
     "data": {
      "text/plain": "[None, None, None, None, None, None, None]"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a closer look at the first publication\n",
    "publications = [scholarly.fill(publication) for publication in my_profile['publications']]\n",
    "\n",
    "[print(f'{idx + 1}: {item[\"bib\"][\"title\"]}') for idx, item in enumerate(publications)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-07-15T12:24:45.274612Z",
     "start_time": "2023-07-15T12:24:31.557410400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "html_escape_table = {\n",
    "    \"&\": \"&amp;\",\n",
    "    '\"': \"&quot;\",\n",
    "    \"'\": \"&apos;\",\n",
    "    \"′\": \"'\",\n",
    "    \"…\": \"...\",\n",
    "    \"“\": \"\\\"\",\n",
    "    \"”\": \"\\\"\"\n",
    "    }\n",
    "\n",
    "def html_escape(text):\n",
    "    \"\"\"Produce entities within text.\"\"\"\n",
    "    return \"\".join(html_escape_table.get(c,c) for c in text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-15T12:34:55.129328800Z",
     "start_time": "2023-07-15T12:34:55.113669600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['title', 'pub_year', 'citation', 'author', 'conference', 'pages', 'abstract'])\n",
      "1: Improving Deep Facial Phenotyping for Ultra-rare Disorder Verification Using Model Ensembles\n",
      "---\n",
      "title: \"Improving Deep Facial Phenotyping for Ultra-rare Disorder Verification Using Model Ensembles\"\n",
      "collection: publications\n",
      "permalink: /publication/2023-13773664767143044663\n",
      "abstract: \"Rare genetic disorders affect more than 6% of the global population. Reaching a diagnosis is challenging because rare disorders are very diverse. Many disorders have recognizable facial features that are hints for clinicians to diagnose patients. Previous work, such as GestaltMatcher, utilized representation vectors produced by a DCNN similar to AlexNet to match patients in high-dimensional feature space to support&quot; unseen&quot; ultra-rare disorders. However, the architecture and dataset used for transfer learning in GestaltMatcher have become outdated. Moreover, a way to train the model for generating better representation vectors for unseen ultra-rare disorders has not yet been studied. Because of the overall scarcity of patients with ultra-rare disorders, it is infeasible to directly train a model on them. Therefore, we first analyzed the influence of replacing GestaltMatcher DCNN with a state-of-the-art face recognition approach, iResNet with ArcFace. Additionally, we experimented with different face recognition datasets for transfer learning. Furthermore, we proposed test-time augmentation, and model ensembles that mix general face verification models and models specific for verifying disorders to improve the disorder verification accuracy of unseen ultra-rare disorders. Our proposed ensemble model achieves state-of-the-art performance on both seen and unseen disorders. Code is available at https://www. github. com/igsb/GestaltMatcher-Arc\"\n",
      "date: 2023\n",
      "venue: 'Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision'\n",
      "paperurl: 'https://openaccess.thecvf.com/content/WACV2023/html/Hustinx_Improving_Deep_Facial_Phenotyping_for_Ultra-Rare_Disorder_Verification_Using_Model_WACV_2023_paper.html'\n",
      "citation: '3'\n",
      "---\n",
      "Rare genetic disorders affect more than 6% of the global population. Reaching a diagnosis is challenging because rare disorders are very diverse. Many disorders have recognizable facial features that are hints for clinicians to diagnose patients. Previous work, such as GestaltMatcher, utilized representation vectors produced by a DCNN similar to AlexNet to match patients in high-dimensional feature space to support&quot; unseen&quot; ultra-rare disorders. However, the architecture and dataset used for transfer learning in GestaltMatcher have become outdated. Moreover, a way to train the model for generating better representation vectors for unseen ultra-rare disorders has not yet been studied. Because of the overall scarcity of patients with ultra-rare disorders, it is infeasible to directly train a model on them. Therefore, we first analyzed the influence of replacing GestaltMatcher DCNN with a state-of-the-art face recognition approach, iResNet with ArcFace. Additionally, we experimented with different face recognition datasets for transfer learning. Furthermore, we proposed test-time augmentation, and model ensembles that mix general face verification models and models specific for verifying disorders to improve the disorder verification accuracy of unseen ultra-rare disorders. Our proposed ensemble model achieves state-of-the-art performance on both seen and unseen disorders. Code is available at https://www. github. com/igsb/GestaltMatcher-Arc\n",
      "\n",
      "[Download paper here](https://openaccess.thecvf.com/content/WACV2023/html/Hustinx_Improving_Deep_Facial_Phenotyping_for_Ultra-Rare_Disorder_Verification_Using_Model_WACV_2023_paper.html)\n",
      "\n",
      "dict_keys(['title', 'pub_year', 'citation', 'author', 'conference', 'pages', 'publisher', 'abstract'])\n",
      "2: Deformable dilated faster R-CNN for universal lesion detection in CT images\n",
      "---\n",
      "title: \"Deformable dilated faster R-CNN for universal lesion detection in CT images\"\n",
      "collection: publications\n",
      "permalink: /publication/2021-10699106320797145046\n",
      "abstract: \"Cancer is a major public health issue and takes the second-highest toll of deaths caused by non-communicable diseases worldwide. Automatically detecting lesions at an early stage is essential to increase the chance of a cure. This study proposes a novel dilated Faster R-CNN with modulated deformable convolution and modulated deformable positive-sensitive region of interest pooling to detect lesions in computer tomography images. A pre-trained VGG-16 is transferred as the backbone of Faster R-CNN, followed by a region proposal network and a region of interest pooling layer to achieve lesion detection. The modulated deformable convolutional layers are employed to learn deformable convolutional filters, while the modulated deformable positive-sensitive region of interest pooling provides an enhanced feature extraction on the feature maps. Moreover, dilated convolutions are combined with the modulated ...\"\n",
      "date: 2021\n",
      "venue: '2021 43rd Annual International Conference of the IEEE Engineering in Medicine &amp; Biology Society (EMBC)'\n",
      "paperurl: 'https://ieeexplore.ieee.org/abstract/document/9631021/'\n",
      "citation: '2'\n",
      "---\n",
      "Cancer is a major public health issue and takes the second-highest toll of deaths caused by non-communicable diseases worldwide. Automatically detecting lesions at an early stage is essential to increase the chance of a cure. This study proposes a novel dilated Faster R-CNN with modulated deformable convolution and modulated deformable positive-sensitive region of interest pooling to detect lesions in computer tomography images. A pre-trained VGG-16 is transferred as the backbone of Faster R-CNN, followed by a region proposal network and a region of interest pooling layer to achieve lesion detection. The modulated deformable convolutional layers are employed to learn deformable convolutional filters, while the modulated deformable positive-sensitive region of interest pooling provides an enhanced feature extraction on the feature maps. Moreover, dilated convolutions are combined with the modulated ...\n",
      "\n",
      "[Download paper here](https://ieeexplore.ieee.org/abstract/document/9631021/)\n",
      "\n",
      "dict_keys(['title', 'pub_year', 'citation', 'author', 'journal', 'abstract'])\n",
      "3: The STOIC2021 COVID-19 AI challenge: applying reusable training methodologies to private data\n",
      "---\n",
      "title: \"The STOIC2021 COVID-19 AI challenge: applying reusable training methodologies to private data\"\n",
      "collection: publications\n",
      "permalink: /publication/2023-3\n",
      "abstract: \"Challenges drive the state-of-the-art of automated medical image analysis. The quantity of public training data that they provide can limit the performance of their solutions. Public access to the training methodology for these solutions remains absent. This study implements the Type Three (T3) challenge format, which allows for training solutions on private data and guarantees reusable training methodologies. With T3, challenge organizers train a codebase provided by the participants on sequestered training data. T3 was implemented in the STOIC2021 challenge, with the goal of predicting from a computed tomography (CT) scan whether subjects had a severe COVID-19 infection, defined as intubation or death within one month. STOIC2021 consisted of a Qualification phase, where participants developed challenge solutions using 2000 publicly available CT scans, and a Final phase, where participants submitted their training methodologies with which solutions were trained on CT scans of 9724 subjects. The organizers successfully trained six of the eight Final phase submissions. The submitted codebases for training and running inference were released publicly. The winning solution obtained an area under the receiver operating characteristic curve for discerning between severe and non-severe COVID-19 of 0.815. The Final phase solutions of all finalists improved upon their Qualification phase solutions.\"\n",
      "date: 2023\n",
      "venue: 'arXiv preprint arXiv:2306.10484'\n",
      "paperurl: 'https://arxiv.org/abs/2306.10484'\n",
      "citation: '0'\n",
      "---\n",
      "Challenges drive the state-of-the-art of automated medical image analysis. The quantity of public training data that they provide can limit the performance of their solutions. Public access to the training methodology for these solutions remains absent. This study implements the Type Three (T3) challenge format, which allows for training solutions on private data and guarantees reusable training methodologies. With T3, challenge organizers train a codebase provided by the participants on sequestered training data. T3 was implemented in the STOIC2021 challenge, with the goal of predicting from a computed tomography (CT) scan whether subjects had a severe COVID-19 infection, defined as intubation or death within one month. STOIC2021 consisted of a Qualification phase, where participants developed challenge solutions using 2000 publicly available CT scans, and a Final phase, where participants submitted their training methodologies with which solutions were trained on CT scans of 9724 subjects. The organizers successfully trained six of the eight Final phase submissions. The submitted codebases for training and running inference were released publicly. The winning solution obtained an area under the receiver operating characteristic curve for discerning between severe and non-severe COVID-19 of 0.815. The Final phase solutions of all finalists improved upon their Qualification phase solutions.\n",
      "\n",
      "[Download paper here](https://arxiv.org/abs/2306.10484)\n",
      "\n",
      "dict_keys(['title', 'pub_year', 'citation', 'author', 'journal', 'abstract'])\n",
      "4: Towards Automated COVID-19 Presence and Severity Classification\n",
      "---\n",
      "title: \"Towards Automated COVID-19 Presence and Severity Classification\"\n",
      "collection: publications\n",
      "permalink: /publication/2023-4\n",
      "abstract: \"COVID-19 presence classification and severity prediction via (3D) thorax computed tomography scans have become important tasks in recent times. Especially for capacity planning of intensive care units, predicting the future severity of a COVID-19 patient is crucial. The presented approach follows state-of-theart techniques to aid medical professionals in these situations. It comprises an ensemble learning strategy via 5-fold cross-validation that includes transfer learning and combines pre-trained 3D-versions of ResNet34 and DenseNet121 for COVID19 classification and severity prediction respectively. Further, domain-specific preprocessing was applied to optimize model performance. In addition, medical information like the infection-lung-ratio, patient age, and sex were included. The presented model achieves an AUC of 79.0% to predict COVID-19 severity, and 83.7% AUC to classify the presence of an infection, which is comparable with other currently popular methods. This approach is implemented using the AUCMEDI framework and relies on well-known network architectures to ensure robustness and reproducibility.\"\n",
      "date: 2023\n",
      "venue: 'arXiv preprint arXiv:2305.08660'\n",
      "paperurl: 'https://arxiv.org/abs/2305.08660'\n",
      "citation: '0'\n",
      "---\n",
      "COVID-19 presence classification and severity prediction via (3D) thorax computed tomography scans have become important tasks in recent times. Especially for capacity planning of intensive care units, predicting the future severity of a COVID-19 patient is crucial. The presented approach follows state-of-theart techniques to aid medical professionals in these situations. It comprises an ensemble learning strategy via 5-fold cross-validation that includes transfer learning and combines pre-trained 3D-versions of ResNet34 and DenseNet121 for COVID19 classification and severity prediction respectively. Further, domain-specific preprocessing was applied to optimize model performance. In addition, medical information like the infection-lung-ratio, patient age, and sex were included. The presented model achieves an AUC of 79.0% to predict COVID-19 severity, and 83.7% AUC to classify the presence of an infection, which is comparable with other currently popular methods. This approach is implemented using the AUCMEDI framework and relies on well-known network architectures to ensure robustness and reproducibility.\n",
      "\n",
      "[Download paper here](https://arxiv.org/abs/2305.08660)\n",
      "\n",
      "dict_keys(['title', 'pub_year', 'citation', 'author', 'journal', 'abstract'])\n",
      "5: GANonymization: A GAN-based Face Anonymization Framework for Preserving Emotional Expressions\n",
      "---\n",
      "title: \"GANonymization: A GAN-based Face Anonymization Framework for Preserving Emotional Expressions\"\n",
      "collection: publications\n",
      "permalink: /publication/2023-5\n",
      "abstract: \"In recent years, the increasing availability of personal data has raised concerns regarding privacy and security. One of the critical processes to address these concerns is data anonymization, which aims to protect individual privacy and prevent the release of sensitive information. This research focuses on the importance of face anonymization. Therefore, we introduce GANonymization, a novel face anonymization framework with facial expression-preserving abilities. Our approach is based on a high-level representation of a face which is synthesized into an anonymized version based on a generative adversarial network (GAN). The effectiveness of the approach was assessed by evaluating its performance in removing identifiable facial attributes to increase the anonymity of the given individual face. Additionally, the performance of preserving facial expressions was evaluated on several affect recognition datasets and outperformed the state-of-the-art method in most categories. Finally, our approach was analyzed for its ability to remove various facial traits, such as jewelry, hair color, and multiple others. Here, it demonstrated reliable performance in removing these attributes. Our results suggest that GANonymization is a promising approach for anonymizing faces while preserving facial expressions.\"\n",
      "date: 2023\n",
      "venue: 'arXiv preprint arXiv:2305.02143'\n",
      "paperurl: 'https://arxiv.org/abs/2305.02143'\n",
      "citation: '0'\n",
      "---\n",
      "In recent years, the increasing availability of personal data has raised concerns regarding privacy and security. One of the critical processes to address these concerns is data anonymization, which aims to protect individual privacy and prevent the release of sensitive information. This research focuses on the importance of face anonymization. Therefore, we introduce GANonymization, a novel face anonymization framework with facial expression-preserving abilities. Our approach is based on a high-level representation of a face which is synthesized into an anonymized version based on a generative adversarial network (GAN). The effectiveness of the approach was assessed by evaluating its performance in removing identifiable facial attributes to increase the anonymity of the given individual face. Additionally, the performance of preserving facial expressions was evaluated on several affect recognition datasets and outperformed the state-of-the-art method in most categories. Finally, our approach was analyzed for its ability to remove various facial traits, such as jewelry, hair color, and multiple others. Here, it demonstrated reliable performance in removing these attributes. Our results suggest that GANonymization is a promising approach for anonymizing faces while preserving facial expressions.\n",
      "\n",
      "[Download paper here](https://arxiv.org/abs/2305.02143)\n",
      "\n",
      "dict_keys(['title', 'pub_year', 'citation', 'author', 'journal', 'abstract'])\n",
      "6: Few-Shot Meta Learning for Recognizing Facial Phenotypes of Genetic Disorders\n",
      "---\n",
      "title: \"Few-Shot Meta Learning for Recognizing Facial Phenotypes of Genetic Disorders\"\n",
      "collection: publications\n",
      "permalink: /publication/2022-6\n",
      "abstract: \"Computer vision-based methods have valuable use cases in precision medicine, and recognizing facial phenotypes of genetic disorders is one of them. Many genetic disorders are known to affect faces&apos; visual appearance and geometry. Automated classification and similarity retrieval aid physicians in decision-making to diagnose possible genetic conditions as early as possible. Previous work has addressed the problem as a classification problem and used deep learning methods. The challenging issue in practice is the sparse label distribution and huge class imbalances across categories. Furthermore, most disorders have few labeled samples in training sets, making representation learning and generalization essential to acquiring a reliable feature descriptor. In this study, we used a facial recognition model trained on a large corpus of healthy individuals as a pre-task and transferred it to facial phenotype recognition. Furthermore, we created simple baselines of few-shot meta-learning methods to improve our base feature descriptor. Our quantitative results on GestaltMatcher Database show that our CNN baseline surpasses previous works, including GestaltMatcher, and few-shot meta-learning strategies improve retrieval performance in frequent and rare classes.\"\n",
      "date: 2022\n",
      "venue: 'arXiv preprint arXiv:2210.12705'\n",
      "paperurl: 'https://arxiv.org/abs/2210.12705'\n",
      "citation: '0'\n",
      "---\n",
      "Computer vision-based methods have valuable use cases in precision medicine, and recognizing facial phenotypes of genetic disorders is one of them. Many genetic disorders are known to affect faces&apos; visual appearance and geometry. Automated classification and similarity retrieval aid physicians in decision-making to diagnose possible genetic conditions as early as possible. Previous work has addressed the problem as a classification problem and used deep learning methods. The challenging issue in practice is the sparse label distribution and huge class imbalances across categories. Furthermore, most disorders have few labeled samples in training sets, making representation learning and generalization essential to acquiring a reliable feature descriptor. In this study, we used a facial recognition model trained on a large corpus of healthy individuals as a pre-task and transferred it to facial phenotype recognition. Furthermore, we created simple baselines of few-shot meta-learning methods to improve our base feature descriptor. Our quantitative results on GestaltMatcher Database show that our CNN baseline surpasses previous works, including GestaltMatcher, and few-shot meta-learning strategies improve retrieval performance in frequent and rare classes.\n",
      "\n",
      "[Download paper here](https://arxiv.org/abs/2210.12705)\n",
      "\n",
      "dict_keys(['title', 'pub_year', 'citation', 'author', 'journal', 'volume', 'number', 'pages', 'publisher', 'abstract'])\n",
      "7: NFB-03. Neurological manifestations in children and adolescents with Neurofibromatosis type 1-Implications for management and surveillance\n",
      "---\n",
      "title: \"NFB-03. Neurological manifestations in children and adolescents with Neurofibromatosis type 1-Implications for management and surveillance\"\n",
      "collection: publications\n",
      "permalink: /publication/2022-7\n",
      "abstract: \" INTRODUCTION: We aimed to (1) characterize the spectrum of clinical phenotypes of NF1 in a random pediatric population, (2) correlate genotype with phenotypic expression for those with a genetic diagnosis, and (3) explore radiological features of NF1 in the central nervous system (CNS) by radiomics analyses to predict clinical course. METHODS: We performed a database search in the hospital information system of the University Children's Hospital between January 2017 and December 2020 for patients with NF1 and evaluated the clinical phenotype by retrospective chart review. RESULTS: 75 children/adolescents were identified with suspicion/clinical diagnosis of NF1 (median age 10.0 years (range, 1.1-22.6); 35 female), confirmatory revised \"diagnostic criteria\" were met in 57 patients at the last follow-up. Per number of documented items, major signs were detected as 73/75 café-au-lait ...\"\n",
      "date: 2022\n",
      "venue: 'Neuro-Oncology'\n",
      "paperurl: 'https://academic.oup.com/neuro-oncology/article-abstract/24/Supplement_1/i128/6601248'\n",
      "citation: '0'\n",
      "---\n",
      " INTRODUCTION: We aimed to (1) characterize the spectrum of clinical phenotypes of NF1 in a random pediatric population, (2) correlate genotype with phenotypic expression for those with a genetic diagnosis, and (3) explore radiological features of NF1 in the central nervous system (CNS) by radiomics analyses to predict clinical course. METHODS: We performed a database search in the hospital information system of the University Children's Hospital between January 2017 and December 2020 for patients with NF1 and evaluated the clinical phenotype by retrospective chart review. RESULTS: 75 children/adolescents were identified with suspicion/clinical diagnosis of NF1 (median age 10.0 years (range, 1.1-22.6); 35 female), confirmatory revised \"diagnostic criteria\" were met in 57 patients at the last follow-up. Per number of documented items, major signs were detected as 73/75 café-au-lait ...\n",
      "\n",
      "[Download paper here](https://academic.oup.com/neuro-oncology/article-abstract/24/Supplement_1/i128/6601248)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "for idx, item in enumerate(publications):\n",
    "    bib = item['bib']\n",
    "    print(bib.keys())\n",
    "    pub_title = bib['title']\n",
    "    print(f'{idx + 1}: {pub_title}')\n",
    "    cites_id = item['cites_id'][0] if 'cites_id' in item else idx + 1\n",
    "    pub_year = bib['pub_year']\n",
    "    pub_abstract = bib['abstract']\n",
    "    pub_conference = bib['conference'] if 'conference' in bib else bib['journal']\n",
    "    pub_url = item['pub_url']\n",
    "    num_citations = item['num_citations']\n",
    "    md_filename = f\"{pub_year}-{cites_id}.md\"\n",
    "    html_filename = f\"{pub_year}-{cites_id}\"\n",
    "\n",
    "    ## YAML variables\n",
    "    md = f\"---\\ntitle: \\\"{pub_title}\\\"\\n\"\n",
    "    md += \"collection: publications\"\n",
    "    md += f\"\\npermalink: /publication/{html_filename}\"\n",
    "    if len(str(pub_abstract)) > 5:\n",
    "        md += f\"\\nabstract: \\\"{html_escape(pub_abstract)}\\\"\"\n",
    "    md += f\"\\ndate: {pub_year}\"\n",
    "    md += f\"\\nvenue: '{html_escape(pub_conference)}'\"\n",
    "    if len(str(pub_url)) > 5:\n",
    "        md += f\"\\npaperurl: '{pub_url}'\"\n",
    "    md += f\"\\ncitation: '{num_citations}'\"\n",
    "    md += \"\\n---\"\n",
    "\n",
    "    ## Markdown description for individual page\n",
    "    if len(str(pub_abstract)) > 5:\n",
    "        md += f\"\\n{html_escape(pub_abstract)}\\n\"\n",
    "    if len(str(pub_url)) > 5:\n",
    "        md += f\"\\n[Download paper here]({pub_url})\\n\"\n",
    "\n",
    "    # md += \"\\nRecommended citation: \" + item.citation\n",
    "    md_filename = os.path.basename(md_filename)\n",
    "    print(md)\n",
    "    with open(\"../_publications/\" + md_filename, 'w') as f:\n",
    "        f.write(md)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-15T12:34:57.131117700Z",
     "start_time": "2023-07-15T12:34:57.068428200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
